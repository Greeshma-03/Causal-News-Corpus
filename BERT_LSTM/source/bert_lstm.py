
import transformers
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
import torch
import numpy as np
import pandas as pd
import logging
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
from textwrap import wrap
from CTB_Case import *

from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F


train_data = data
# For printing to the log
logger = logging.getLogger(__name__)

# Loading the data used for training and testing
val_data = pd.read_csv("dev_subtask1.csv")
test_data = pd.read_csv("test_subtask1.csv")

# Transformer model name used
model_name = 'bert-base-cased'

tokenizer = BertTokenizer.from_pretrained(model_name)
# Adding special tokens
encoding = tokenizer.encode_plus(
    train_data,
    max_length=32,
    add_special_tokens=True,      # Adding the '[CLS]' and '[SEP]'
    return_token_type_ids=False,
    pad_to_max_length=True,
    return_attention_mask=True,
    return_tensors='pt',          # PyTorch tensors returning
)
# Trying to get GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(encoding.keys())


# Each sentence is classified only into causal and non-causal
class_names = ['causal', 'non-causal']


# Getting the Input Tokens and attention mask to pass to BERT

print(len(encoding['input_ids'][0]))
encoding['input_ids'][0]

print(len(encoding['attention_mask'][0]))
encoding['attention_mask']

token_lens = []

for txt in df.content:
    tokens = tokenizer.encode(txt, max_length=512)
    token_lens.append(len(tokens))

# Loading the BERT model

bert_model = BertModel.from_pretrained(model_name)

# Testing for the output
last_hidden_state, pooled_output = bert_model(
    input_ids=encoding['input_ids'],
    attention_mask=encoding['attention_mask']
)

# Checking the lengths and shape of the BERT hidden states and pooled output
print(last_hidden_state.shape)
print(bert_model.config.hidden_size)
print(pooled_output.shape)


## MODEL ##

class CausalClassifier(nn.Module):

    def __init__(self, hidden_dim, n_classes=2):
        super(CausalClassifier, self).__init__()
        # Intialising the pre-trained BERT Model
        self.bert = BertModel.from_pretrained(model_name)
        # Drop out layers added to avoid overfitting
        self.drop = nn.Dropout(p=0.3)
        self.dropout = nn.Dropout(0.5)
        self.embedding = nn.Embedding(
            self.input_size, self.hidden_dim, padding_idx=0)
        # LSTM layer to take in the embeddings generated by BERT
        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim,
                            num_layers=self.LSTM_layers, batch_first=True)
        # Two linear layers added to output a 1d vector represneting probability belonging to 1 class
        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=256)
        self.fc2 = nn.Linear(256, 1)

    def forward(self, input_ids, attention_mask):

        # Word embeddings of tokens are generated by bert along with pooled output
        embeddings, pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Initialising the hidden and cell states with zeros
        h = torch.zeros(
            (self.LSTM_layers, embeddings.size(0), self.hidden_dim))
        c = torch.zeros(
            (self.LSTM_layers, embeddings.size(0), self.hidden_dim))

        # Adding dropout layers
        out = self.drop(embeddings)

        # output of BERT Genreated embeddings passed to LSTM layer
        out, (hidden, cell) = self.lstm(out, (h, c))

        # Dropoped out some neurons from network randomly and applied Relu non-linear layer
        # Followed by sigmoid activation
        out = self.dropout(out)
        out = torch.relu_(self.fc1(out[:, -1, :]))
        out = self.dropout(out)
        out = torch.sigmoid(self.fc2(out))

        return out


hidden_dim = 50

model = CausalClassifier(hidden_dim)
model = model.to(device)

input_ids = train_data['input_ids'].to(device)
attention_mask = train_data['attention_mask'].to(device)

print(input_ids.shape)
print(attention_mask.shape)


F.softmax(model(input_ids, attention_mask), dim=1)


EPOCHS = 20
# ADamW optimizer from transformer for better perfomance than using normal Adam optimizer
optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data) * EPOCHS


# Using builtin linear scheduler from transofmer to incline with BERT for better performance
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)
# Loss criterion
loss_fn = nn.CrossEntropyLoss().to(device)


# Printing the Evaluation metrics
def evaluation(self):
    logger.info("***** eval metrics *****")
    # As we don't backpropogate during this eval phase
    predictions = []
    self.model.eval()
    with torch.no_grad():
        for x_batch, y_batch in self.loader_test:
            x = x_batch.type(torch.LongTensor)
            y = y_batch.type(torch.FloatTensor)
            ## here we are predicting the results
            y_pred = self.model(x)
            predictions += list(y_pred.detach().numpy())

    return predictions

# Calculating the accuracy
def calculate_accuray(grand_truth, predictions):
    logger.info("***** Predict *****")
    true_positives = 0
    true_negatives = 0

    ## checking the porbabilites to calculate the the truepositives and true negatives

    for true, pred in zip(grand_truth, predictions):
        if (pred > 0.5) and (true == 1):
            true_positives += 1
        elif (pred < 0.5) and (true == 0):
            true_negatives += 1
        else:
            pass

    return (true_positives+true_negatives) / len(grand_truth)


# Training phase
def train_epoch(
        model,
        data_loader,
        loss_fn,
        optimizer,
        device,
        scheduler,
        n_examples):
    model = model.train()

    losses = []
    correct_predictions = 0
    predictions=[]

    for d in data_loader:


        # here Input IDs are  simply mappings between tokens and their respective IDs and
        # The attention mask is to prevent the model from looking at padding tokens.


        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        targets = d["targets"].to(device)

        # output from BERT_lstm model along with added network
        y_pred = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        ## Calculating the loss of predicted outputs

        loss = F.binary_cross_entropy(y_pred, targets)

           # Stepping optimizer 
        optimizer.zero_grad()

        ## Backward propgation for updating the weights

        loss.backward()


        optimizer.step()

        predictions += list(y_pred.squeeze().detach().numpy())

        test_predictions = model.evaluation()

        ## finding the model train and test accuracy

        train_accuracy = model.calculate_accuray(
            model.y_train, predictions)
        test_accuracy = model.calculate_accuray(
            model.y_test, test_predictions)

    return train_accuracy, test_accuracy


## Evaluating the model

def eval_model(model, data_loader, loss_fn, device, n_examples):

    model = model.eval()

    losses = []
    correct_predictions = 0

      
    # As we don't backpropogate during this eval phase
    with torch.no_grad():
        for d in data_loader:
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            targets = d["targets"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

               ## here we are predicting the results
            _, preds = torch.max(outputs, dim=1)

              ## Calculating the loss of the predicted outputs and storing it

            loss = loss_fn(outputs, targets)

            correct_predictions += torch.sum(preds == targets)
            losses.append(loss.item())

    logger.info("***** eval metrics *****")
    return correct_predictions.double() / n_examples, np.mean(losses)


history = defaultdict(list)
best_accuracy = 0


 # Now performing the training on data and evaluating the model

for epoch in range(EPOCHS):

    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)

    train_acc, train_loss = train_epoch(
        model,
        train_data,
        loss_fn,
        optimizer,
        device,
        scheduler,
        len(train_data)
    )

    print(f'Train loss {train_loss} accuracy {train_acc}')

    val_acc, val_loss = eval_model(
        model,
        val_data,
        loss_fn,
        device,
        len(val_data)
    )

    print(f'Val   loss {val_loss} accuracy {val_acc}')
    print()


    ## savingthe accuracy and loss

    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)

     ## storing the best accuracy

    if val_acc > best_accuracy:
        torch.save(model.state_dict(), 'best_model_state.bin')
        best_accuracy = val_acc


## plotting the accuracy of the models

plt.plot(history['train_acc'], label='train accuracy')
plt.plot(history['val_acc'], label='validation accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1])


test_acc, _ = eval_model(
    model,
    test_data,
    loss_fn,
    device,
    len(test_data)
)

test_acc.item()

# TESTING


def get_predictions(model, data_loader):
    model = model.eval()

    review_texts = []
    predictions = []
    prediction_probs = []
    real_values = []

    with torch.no_grad():
        for d in data_loader:

              ## taking each sentence in data and extracting the input_ids and attention_masks

            texts = d["text"]
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            targets = d["targets"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

              ## predictions from trained model
            _, preds = torch.max(outputs, dim=1)

            ## adding the softmax layer for the classification as activation function

            probs = F.softmax(outputs, dim=1)

            ## Storing the predictions

            review_texts.extend(texts)
            predictions.extend(preds)
            prediction_probs.extend(probs)
            real_values.extend(targets)

    predictions = torch.stack(predictions).cpu()
    prediction_probs = torch.stack(prediction_probs).cpu()
    real_values = torch.stack(real_values).cpu()
    logger.info("***** Predict *****")

    return review_texts, predictions, prediction_probs, real_values


y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(
    model,
    test_data
)

# Printing the stats in terminal
logger.info("***** Causal News Corpus(Team :Thunderbolts) *****")
print(classification_report(y_test, y_pred, target_names=class_names))


# Displaying the confusion matrix for the data we have used so far in evaluation phase
def show_confusion_matrix(confusion_matrix):
    hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
    hmap.yaxis.set_ticklabels(
        hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
    hmap.xaxis.set_ticklabels(
        hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
    plt.ylabel('True')
    plt.xlabel('Predicted')



# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
show_confusion_matrix(df_cm)
